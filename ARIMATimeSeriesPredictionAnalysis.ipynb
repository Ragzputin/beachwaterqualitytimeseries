{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as et\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.graphics import tsaplots\n",
    "\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "#from statsmodels.stats.proportion import proportion_ztest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Acquire Data From CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beach_complete = pd.read_csv('beach_complete.csv',delimiter=',',header=0,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beach_complete.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beach_clean = beach_complete.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beach_clean.apply(pd.to_numeric);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(6, 2, figsize=(20,40))\n",
    "cols = beach_clean.columns\n",
    "for i in range(6):\n",
    "    for j in range(2):\n",
    "        if j == 1 and i == 5:\n",
    "            break\n",
    "        elif j == 0:\n",
    "            ax[i][j].hist(beach_clean[cols[i]].values, bins=100, histtype='stepfilled')\n",
    "            ax[i][j].set(xlabel='E.coli counts',ylabel='Frequency',title=cols[i])\n",
    "            #set_trace()\n",
    "        else:\n",
    "            ax[i][j].hist(beach_clean[cols[i+6]].values, bins=100, histtype='stepfilled')\n",
    "            ax[i][j].set(xlabel='E.coli counts',ylabel='Frequency',title=cols[i+6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Remove unwanted seasonality & check stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of series and titles to assist in graphing the e.Coli counts for each beach\n",
    "list_series = [beach_complete[col].dropna().astype(int) for col in beach_complete]\n",
    "list_titles = [x for x in beach_complete.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove any NaN values from each series\n",
    "#for series in list_series:\n",
    "#    series.dropna().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the list titles\n",
    "list_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Differencing of the data to remove the unwanted seasonality and check for stationarity\n",
    "diff_series = []\n",
    "days_in_year = 365\n",
    "for series in list_series:\n",
    "    diff = []\n",
    "    for i in range(days_in_year, series.size):\n",
    "        value = np.abs(series[i] - series[i - days_in_year])\n",
    "        diff.append(value)\n",
    "    diff_series.append(pd.Series(diff))\n",
    "diff_series[0]\n",
    "\n",
    "#Plot all diffs\n",
    "f, ax = plt.subplots(11, 2, figsize=(20,50))\n",
    "row_size = 2\n",
    "col_size = 11\n",
    "for i in range(col_size):\n",
    "    for j in range(row_size):\n",
    "        if j == 0:\n",
    "            ax[i][j].plot(diff_series[i].rolling(window=50).mean())\n",
    "            ax[i][j].set(xlabel='Differenced Datapoints', ylabel='e. Coli Differenced Avg', \n",
    "                         title = list_titles[i])\n",
    "        else:\n",
    "            ax[i][j].plot(diff_series[i].rolling(window=50).std())\n",
    "            ax[i][j].set(xlabel='Differenced Datapoints', ylabel='e. Coli Differenced Std Dev', \n",
    "                         title = list_titles[i])\n",
    "plt.subplots_adjust(bottom=-0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above differenced graph looks quite stationary considering it does not have an upward or downward inclination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped = zip(list_series, list_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tuples = list(zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Just a repeat of above differenced data - this code is unnecessary\n",
    "def plot_stationary(list_tuples):\n",
    "    for tup in list_tuples:\n",
    "        #print(tup[1])\n",
    "        series = tup[0]\n",
    "        title = tup[1]\n",
    "        str_title = 'Rolling Avg & Std Dev ' + title\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10,6))\n",
    "        series.rolling(window=12).mean().plot(style='o', ax=ax1)\n",
    "        series.rolling(window=12).std().plot(style='o',ax=ax2)\n",
    "        ax1.set(xlabel='Time (Years)', ylabel='e.Coli Counts rolling avg', title = str_title)\n",
    "        ax2.set(xlabel='Time (Years)', ylabel='e.Coli Counts rolling std dev')\n",
    "        ax1.axhline(y=25, color='r', linestyle='--')\n",
    "        ax2.axhline(y=25, color='r', linestyle='--')\n",
    "        plt.show();\n",
    "\n",
    "plot_stationary(list_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The above stionarity checks for both rolling average and rolling std dev show that the e.Coli count data are stationary over the past 10 years at Sunnyside beach. There are no downward/uppward trends in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: ACF and PACF plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_acf_pacf(list_tuples):\n",
    "    fig, ax = plt.subplots(11, 2, figsize=(20,40))\n",
    "    for i in range(len(list_tuples)):\n",
    "        for j in range(len(list_tuples[i])-1):\n",
    "            str_title_acf = \"Autocorrelation \" + list_tuples[i][1]\n",
    "            str_title_pacf = \"Partial Autocorrelation  \" + list_tuples[i][1]\n",
    "            plot_acf(list_tuples[i][0], lags=50, title=str_title_acf, ax=ax[i][j])\n",
    "            plot_pacf(list_tuples[i][0], lags=50, title=str_title_pacf, ax=ax[i][j+1])\n",
    "    \n",
    "    plt.show();\n",
    "\n",
    "plot_acf_pacf(list_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all 11 beaches, it seems that with a lag=6 there will be very little partial correlation. So we assume lag=6 for both our AR and MA parameters for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Train and Test ARIMA model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get 70% train, 30% test datasets for all beaches data\n",
    "def get_train_test(tuples):\n",
    "    train_data, test_data = [], []\n",
    "    train_titles, test_titles = [], []\n",
    "    #err_check_ctr = 0\n",
    "    for tup in tuples:\n",
    "        training_size = int(tup[0].values.size * 0.7)\n",
    "        training_set = tup[0][:training_size]\n",
    "        testing_set = tup[0][training_size:tup[0].values.size]\n",
    "        training_title = 'Training ' + tup[1]\n",
    "        testing_title = 'Testing ' + tup[1]\n",
    "        \n",
    "        #check if there are any null values in train/test datasets\n",
    "        #err_check_ctr = 1\n",
    "        #if err_check_ctr == 1:\n",
    "        #    assert training_set.notnull().values.any(), 'Null Values Found in Training Dataset!'\n",
    "        #    assert testing_set.notnull().values.any(), 'Null Values Found in Testing Dataset!'\n",
    "        #    err_check_ctr = 0\n",
    "        \n",
    "        train_data.append(training_set)\n",
    "        test_data.append(testing_set)\n",
    "        train_titles.append(training_title)\n",
    "        test_titles.append(testing_title)\n",
    "    \n",
    "    return (pd.DataFrame(train_data).transpose().dropna(), pd.DataFrame(test_data).transpose().dropna())\n",
    "\n",
    "train_test_tuple = get_train_test(list_tuples)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_tuple[0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_tuple[1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the ARIMA model with train data and create a model just to predict\n",
    "#the N+1 entry using the trained data\n",
    "def single_model_arima(train_data, train_titles):\n",
    "    arr = np.zeros((1, 4))\n",
    "    arima_model_fits = []\n",
    "    train_titles_arr = np.array([train_titles])\n",
    "    for beach in train_data:\n",
    "        amodel = ARIMA(endog=beach.values, order=(6,1,1)) #initialize the ARIMA model with training data\n",
    "        amodel_fit = amodel.fit(disp=1,maxiter=500) #fit the ARIMA model to the training data\n",
    "        p, err, conf = amodel_fit.forecast() #ARIMA model forecast\n",
    "        \n",
    "        params = np.append(p,[err[0],conf[0][0],conf[0][1]]) #Get the parameters (p, err, conf) into an array\n",
    "        arima_model_fits.append(amodel_fit) #Get all the fit models so we can get their summaries later on\n",
    "        #set_trace()\n",
    "        arr = np.append(arr,[params], axis=0) #Append params array to main array\n",
    "    \n",
    "    arr = np.delete(arr, (0), axis=0)\n",
    "    arr = np.concatenate((arr, train_titles_arr.T), axis=1)\n",
    "    \n",
    "    return pd.DataFrame(arr[:,:4], index=arr[:,4].T, columns=['N+1 Prediction','Std Error','CI Low','CI High'])\n",
    "\n",
    "train_data = [train_test_tuple[0][col] for col in train_test_tuple[0]]\n",
    "train_titles = [col for col in train_test_tuple[0]]\n",
    "train_results = single_model_arima(train_data, train_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change all values in training results to float64\n",
    "train_results = train_results.applymap(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check percentage differences between predictions and actual test values\n",
    "abs_diff = np.abs(train_results['N+1 Prediction'] - train_test_tuple[1].iloc[0])\n",
    "abs_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='darkgrid')\n",
    "ax = sns.boxplot(x=abs_diff)\n",
    "ax.set(xlabel='Absolute Difference', title='Absolute Differences after ARIMA Single Data Point Prediction');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above percentage errors varies from -34% (lowest) to 328% (highest). These values represents the difference between predicted vs actual e.Coli count. For smaller differences, it can be said that the model fits well. However for larger differences, more predictions are needed based on the arima model.\n",
    "\n",
    "In following section, more predictions are done for all beaches except for MCurtis, Centre, Woodbine, and Kewbalmy as the percentage error is small for these four. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create more predictions using arima model TODO\n",
    "\n",
    "#def multi_model_arima(train_test_tuple, train_titles):\n",
    "#    arr = np.zeros((1, train_test_tuple[1]['MCurtis-1'].size))\n",
    "    #print(arr)\n",
    "#    train_titles_arr = np.array([train_titles])\n",
    "#    for col in train_test_tuple[0]:\n",
    "#        pred=[]\n",
    "#        hist=list(train_test_tuple[0][col].values)\n",
    "\n",
    "#        for i in range(train_test_tuple[1][col].size):\n",
    "            #step 0: Initialize the ARIMA model\n",
    "#            arima_model = ARIMA(endog=hist, order=(1,1,1))\n",
    "            #step 1: Fit the arima model\n",
    "            #set_trace()\n",
    "#            arima_fit = arima_model.fit(disp=1, maxiter=100)\n",
    "            #step 2: Forecast with the arima model\n",
    "#            p = arima_fit.forecast()[0][0]\n",
    "            #step 3: Add predicted value (p) to prediction list\n",
    "            #set_trace()\n",
    "#            pred.append(p)\n",
    "            #step 4: Then add the current value to the history\n",
    "#            hist.append(train_test_tuple[1][col].values[i])\n",
    "            #step 5: Append the predicted results to the return array\n",
    "        \n",
    "        #set_trace()\n",
    "#        pred_array = np.array(pred)\n",
    "#        arr = np.append(arr, [pred], axis=0)\n",
    "    \n",
    "#    arr = np.delete(arr, (0), axis=0)\n",
    "#    print(arr)\n",
    "        #arr = np.concatenate((arr, pred), axis=0)\n",
    "        #print(arr)\n",
    "        #break\n",
    "               \n",
    "#train_data = [train_test_tuple[0][col] for col in train_test_tuple[0]]\n",
    "#train_titles = [col for col in train_test_tuple[0]]\n",
    "#predictions = [pred1, pred10, pred11, pred2, pred3, pred4, pred5, pred6, pred7, pred8, pred9]\n",
    "#print(train_test_tuple[1]['MCurtis-1'].size)\n",
    "#multi_model_arima(train_test_tuple, train_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mcurtis=[]\n",
    "hist_mcurtis=list(train_test_tuple[0]['MCurtis-1'].values)\n",
    "\n",
    "for i in range(train_test_tuple[1]['MCurtis-1'].size):\n",
    "    arima_model_mcurtis = ARIMA(endog=hist_mcurtis, order=(6,1,1))\n",
    "    arima_fit_mcurtis = arima_model_mcurtis.fit()\n",
    "    p_mcurtis = arima_fit_mcurtis.forecast()[0][0]\n",
    "    pred_mcurtis.append(p_mcurtis)\n",
    "    hist_mcurtis.append(train_test_tuple[1]['MCurtis-1'].values[i]) #pass 611"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sunnyside=[]\n",
    "hist_sunnyside=list(train_test_tuple[0]['Sunnyside-2'].values)\n",
    "\n",
    "for i in range(train_test_tuple[1]['Sunnyside-2'].size):\n",
    "    arima_model_sunnyside = ARIMA(endog=hist_sunnyside, order=(6,1,1))\n",
    "    arima_fit_sunnyside = arima_model_sunnyside.fit()\n",
    "    p_sunnyside = arima_fit_sunnyside.forecast()[0][0]\n",
    "    pred_sunnyside.append(p_sunnyside)\n",
    "    hist_sunnyside.append(train_test_tuple[1]['Sunnyside-2'].values[i]) #pass 611"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_hanlans=[]\n",
    "hist_hanlans=list(train_test_tuple[0]['Hanlans-3'].values)\n",
    "\n",
    "for i in range(train_test_tuple[1]['Hanlans-3'].size):\n",
    "    arima_model_hanlans = ARIMA(endog=hist_hanlans, order=(6,1,1))\n",
    "    arima_fit_hanlans = arima_model_hanlans.fit()\n",
    "    p_hanlans = arima_fit_hanlans.forecast()[0][0]\n",
    "    pred_hanlans.append(p_hanlans)\n",
    "    hist_hanlans.append(train_test_tuple[1]['Hanlans-3'].values[i]) #pass 611"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_gibraltar=[]\n",
    "hist_gibraltar=list(train_test_tuple[0]['Gibraltar-4'].values)\n",
    "\n",
    "for i in range(train_test_tuple[1]['Gibraltar-4'].size):\n",
    "    arima_model_gibraltar = ARIMA(endog=hist_gibraltar, order=(6,1,1))\n",
    "    arima_fit_gibraltar = arima_model_gibraltar.fit()\n",
    "    p_gibraltar = arima_fit_gibraltar.forecast()[0][0]\n",
    "    pred_gibraltar.append(p_gibraltar)\n",
    "    hist_gibraltar.append(train_test_tuple[1]['Gibraltar-4'].values[i]) #pass 611"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_centre=[]\n",
    "hist_centre=list(train_test_tuple[0]['Centre-5'].values)\n",
    "\n",
    "for i in range(train_test_tuple[1]['Centre-5'].size):\n",
    "    arima_model_centre = ARIMA(endog=hist_centre, order=(6,1,1))\n",
    "    arima_fit_centre = arima_model_centre.fit()\n",
    "    p_centre = arima_fit_centre.forecast()[0][0]\n",
    "    pred_centre.append(p_centre)\n",
    "    hist_centre.append(train_test_tuple[1]['Centre-5'].values[i]) #pass 611"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_wards=[]\n",
    "hist_wards=list(train_test_tuple[0]['Wards-6'].values)\n",
    "\n",
    "for i in range(train_test_tuple[1]['Wards-6'].size):\n",
    "    arima_model_wards = ARIMA(endog=hist_wards, order=(6,1,1))\n",
    "    arima_fit_wards = arima_model_wards.fit()\n",
    "    p_wards = arima_fit_wards.forecast()[0][0]\n",
    "    pred_wards.append(p_wards)\n",
    "    hist_wards.append(train_test_tuple[1]['Wards-6'].values[i]) #pass 611"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cherry=[]\n",
    "hist_cherry=list(train_test_tuple[0]['Cherry-7'].values)\n",
    "\n",
    "for i in range(train_test_tuple[1]['Cherry-7'].size):\n",
    "    arima_model_cherry = ARIMA(endog=hist_cherry, order=(6,1,1))\n",
    "    arima_fit_cherry = arima_model_cherry.fit()\n",
    "    p_cherry = arima_fit_cherry.forecast()[0][0]\n",
    "    pred_cherry.append(p_cherry)\n",
    "    hist_cherry.append(train_test_tuple[1]['Cherry-7'].values[i]) #pass 611"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_woodbine=[]\n",
    "hist_woodbine=list(train_test_tuple[0]['Woodbine-8'].values)\n",
    "\n",
    "for i in range(train_test_tuple[1]['Woodbine-8'].size):\n",
    "    arima_model_woodbine = ARIMA(endog=hist_woodbine, order=(6,1,1))\n",
    "    arima_fit_woodbine = arima_model_woodbine.fit()\n",
    "    p_woodbine = arima_fit_woodbine.forecast()[0][0]\n",
    "    pred_woodbine.append(p_woodbine)\n",
    "    hist_woodbine.append(train_test_tuple[1]['Woodbine-8'].values[i]) #pass 611"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_kewbalmy=[]\n",
    "hist_kewbalmy=list(train_test_tuple[0]['KewBalmy-9'].values)\n",
    "\n",
    "for i in range(train_test_tuple[1]['KewBalmy-9'].size):\n",
    "    arima_model_kewbalmy = ARIMA(endog=hist_kewbalmy, order=(6,1,1))\n",
    "    arima_fit_kewbalmy = arima_model_kewbalmy.fit()\n",
    "    p_kewbalmy = arima_fit_kewbalmy.forecast()[0][0]\n",
    "    pred_kewbalmy.append(p_kewbalmy)\n",
    "    hist_kewbalmy.append(train_test_tuple[1]['KewBalmy-9'].values[i]) #pass 611"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bluffers=[]\n",
    "hist_bluffers=list(train_test_tuple[0]['Bluffers-10'].values)\n",
    "\n",
    "for i in range(train_test_tuple[1]['Bluffers-10'].size):\n",
    "    arima_model_bluffers = ARIMA(endog=hist_bluffers, order=(6,1,1))\n",
    "    arima_fit_bluffers = arima_model_bluffers.fit()\n",
    "    p_bluffers = arima_fit_bluffers.forecast()[0][0]\n",
    "    pred_bluffers.append(p_bluffers)\n",
    "    hist_bluffers.append(train_test_tuple[1]['Bluffers-10'].values[i]) #pass 611"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rouge=[]\n",
    "hist_rouge=list(train_test_tuple[0]['Rouge-11'].values)\n",
    "\n",
    "for i in range(train_test_tuple[1]['Rouge-11'].size):\n",
    "    arima_model_rouge = ARIMA(endog=hist_rouge, order=(6,1,1))\n",
    "    arima_fit_rouge = arima_model_rouge.fit()\n",
    "    p_rouge = arima_fit_rouge.forecast()[0][0]\n",
    "    pred_rouge.append(p_rouge)\n",
    "    hist_rouge.append(train_test_tuple[1]['Rouge-11'].values[i]) #pass 611"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph the predictions and the actual values of the test data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(pred_mcurtis, 'r-')\n",
    "plt.plot(train_test_tuple[1]['MCurtis-1'].values, '--')\n",
    "plt.title('Marie Curtis Beach e.Coli counts (2016-2018) ARIMA prediction versus actual values')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(pred_sunnyside, 'r-')\n",
    "plt.plot(train_test_tuple[1]['Sunnyside-2'].values, '--')\n",
    "plt.title('Sunnyside Beach e.Coli counts (2016-2018) ARIMA prediction versus actual values')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(pred_hanlans, 'r-')\n",
    "plt.plot(train_test_tuple[1]['Hanlans-3'].values, '--')\n",
    "plt.title('Hanlans Point e.Coli counts (2016-2018) ARIMA prediction versus actual values')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(pred_gibraltar, 'r-')\n",
    "plt.plot(train_test_tuple[1]['Gibraltar-4'].values, '--')\n",
    "plt.title('Gibraltar Beach e.Coli counts (2016-2018) ARIMA prediction versus actual values')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(pred_centre, 'r-')\n",
    "plt.plot(train_test_tuple[1]['Centre-5'].values, '--')\n",
    "plt.title('Centre Island Beach e.Coli counts (2016-2018) ARIMA prediction versus actual values')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(pred_wards, 'r-')\n",
    "plt.plot(train_test_tuple[1]['Wards-6'].values, '--')\n",
    "plt.title('Wards Beach e.Coli counts (2016-2018) ARIMA prediction versus actual values')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(pred_cherry, 'r-')\n",
    "plt.plot(train_test_tuple[1]['Cherry-7'].values, '--')\n",
    "plt.title('Cherry Beach e.Coli counts (2016-2018) ARIMA prediction versus actual values')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(pred_woodbine, 'r-')\n",
    "plt.plot(train_test_tuple[1]['Woodbine-8'].values, '--')\n",
    "plt.title('Woodbine Beach e.Coli counts (2016-2018) ARIMA prediction versus actual values')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(pred_kewbalmy, 'r-')\n",
    "plt.plot(train_test_tuple[1]['KewBalmy-9'].values, '--')\n",
    "plt.title('KewBalmy Beach e.Coli counts (2016-2018) ARIMA prediction versus actual values')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(pred_bluffers, 'r-')\n",
    "plt.plot(train_test_tuple[1]['Bluffers-10'].values, '--')\n",
    "plt.title('Bluffers Beach e.Coli counts (2016-2018) ARIMA prediction versus actual values')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.plot(pred_rouge, 'r-')\n",
    "plt.plot(train_test_tuple[1]['Rouge-11'].values, '--')\n",
    "plt.title('Rouge Beach e.Coli counts (2016-2018) ARIMA prediction versus actual values')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eyeballing the graphs above, the ARIMA model seems to have done a good job at predicting the last 30% of the Sunnyside beach e.Coli count (2016-2018) based on the first 70% (2009-2016). This needs to be verified by comparing the rmse values with all the other beaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_square_err_mcurtis = mean_squared_error(pred_mcurtis, train_test_tuple[1]['MCurtis-1'].values)\n",
    "print(\"Mean square error MCurtis:\", mean_square_err_mcurtis)\n",
    "\n",
    "mean_square_err_sunnyside = mean_squared_error(pred_sunnyside, train_test_tuple[1]['Sunnyside-2'].values)\n",
    "print(\"Mean square error Sunnyside:\", mean_square_err_sunnyside)\n",
    "\n",
    "mean_square_err_hanlans = mean_squared_error(pred_hanlans, train_test_tuple[1]['Hanlans-3'].values)\n",
    "print(\"Mean square error Hanlans:\", mean_square_err_hanlans)\n",
    "\n",
    "mean_square_err_gibraltar = mean_squared_error(pred_gibraltar, train_test_tuple[1]['Gibraltar-4'].values)\n",
    "print(\"Mean square error Gibraltar:\", mean_square_err_gibraltar)\n",
    "\n",
    "mean_square_err_centre = mean_squared_error(pred_centre, train_test_tuple[1]['Centre-5'].values)\n",
    "print(\"Mean square error Centre:\", mean_square_err_centre)\n",
    "\n",
    "mean_square_err_wards = mean_squared_error(pred_wards, train_test_tuple[1]['Wards-6'].values)\n",
    "print(\"Mean square error Wards:\", mean_square_err_wards)\n",
    "\n",
    "mean_square_err_cherry = mean_squared_error(pred_cherry, train_test_tuple[1]['Cherry-7'].values)\n",
    "print(\"Mean square error Cherry:\", mean_square_err_cherry)\n",
    "\n",
    "mean_square_err_woodbine = mean_squared_error(pred_woodbine, train_test_tuple[1]['Woodbine-8'].values)\n",
    "print(\"Mean square error Woodbine:\", mean_square_err_woodbine)\n",
    "\n",
    "mean_square_err_kewbalmy = mean_squared_error(pred_kewbalmy, train_test_tuple[1]['KewBalmy-9'].values)\n",
    "print(\"Mean square error Kewbalmy:\", mean_square_err_kewbalmy)\n",
    "\n",
    "mean_square_err_bluffers = mean_squared_error(pred_bluffers, train_test_tuple[1]['Bluffers-10'].values)\n",
    "print(\"Mean square error Bluffers:\", mean_square_err_bluffers)\n",
    "\n",
    "mean_square_err_rouge = mean_squared_error(pred_rouge, train_test_tuple[1]['Rouge-11'].values)\n",
    "print(\"Mean square error Rouge:\", mean_square_err_rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_mcurtis = np.sqrt(mean_square_err_mcurtis)\n",
    "rmse_sunnyside = np.sqrt(mean_square_err_sunnyside)\n",
    "rmse_hanlans = np.sqrt(mean_square_err_hanlans)\n",
    "rmse_gibraltar = np.sqrt(mean_square_err_gibraltar)\n",
    "rmse_centre = np.sqrt(mean_square_err_centre)\n",
    "rmse_wards = np.sqrt(mean_square_err_wards)\n",
    "rmse_cherry = np.sqrt(mean_square_err_cherry)\n",
    "rmse_woodbine = np.sqrt(mean_square_err_woodbine)\n",
    "rmse_kewbalmy = np.sqrt(mean_square_err_kewbalmy)\n",
    "rmse_bluffers = np.sqrt(mean_square_err_bluffers)\n",
    "rmse_rouge = np.sqrt(mean_square_err_rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE MCurtis:\", rmse_mcurtis)\n",
    "print(\"RMSE Sunnyside:\", rmse_sunnyside)\n",
    "print(\"RMSE Hanlans:\", rmse_hanlans)\n",
    "print(\"RMSE Gibraltar:\", rmse_gibraltar)\n",
    "print(\"RMSE Centre:\", rmse_centre)\n",
    "print(\"RMSE Wards:\", rmse_wards)\n",
    "print(\"RMSE Cherry:\", rmse_cherry)\n",
    "print(\"RMSE Woodbine:\", rmse_woodbine)\n",
    "print(\"RMSE Kewbalmy:\", rmse_kewbalmy)\n",
    "print(\"RMSE Bluffers:\", rmse_bluffers)\n",
    "print(\"RMSE Rouge:\", rmse_rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmses = np.array([rmse_mcurtis, rmse_sunnyside, rmse_hanlans, rmse_gibraltar, \n",
    "         rmse_centre, rmse_wards, rmse_cherry, rmse_woodbine, rmse_kewbalmy, rmse_bluffers, rmse_rouge])\n",
    "\n",
    "rmse_df = pd.DataFrame(rmses, columns=['RMSE'])\n",
    "rmse_df\n",
    "\n",
    "sns.set(style='darkgrid')\n",
    "ax = sns.boxplot(x=rmse_df['RMSE'])\n",
    "ax.set(xlabel='RMSE', title='RMSE values after ARIMA Expanded Prediction');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: ARIMA Model Test Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_fit_mcurtis.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_fit_sunnyside.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_fit_hanlans.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_fit_gibraltar.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_fit_centre.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_fit_wards.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_fit_cherry.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_fit_woodbine.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_fit_kewbalmy.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_fit_bluffers.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_fit_rouge.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE values closer to 0 indicates better fit. \n",
    "\n",
    "From above RMSE values, it is clear that ARIMA predictions were better for following beaches: Gibraltar, Centre, and Cherry. \n",
    "\n",
    "For Sunnyside, MCurtis and Kewbalmy, the ARIMA predictions are not reliable and hence other models should be used for better predictions. \n",
    "\n",
    "ARIMA did not perform so well for beaches Wards, Woodbine, and Rouge as well.\n",
    "\n",
    "For all beaches, the ARIMA model with parameters (p,d,q) = (6,1,1) did not predict well on sudden spikes in e.Coli counts. This could possibly be because of the sudden changes that these spikes bring compared to the base values seen at the bottom of the above graphs. This will be explored in future with more fine-tuned ARIMA models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
